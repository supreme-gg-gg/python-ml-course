{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# gym toolkit is a collection of environments \n",
    "# used to test reinforcement learning\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Technicall you would use PIL and pyvirtualdisplay to handle the graphics but they are bugging lmao_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000\n",
    "GAMMA = 0.995\n",
    "ALPHA = 1e-3\n",
    "NUM_STEPS_FOR_UPDATE = 4\n",
    "TAU = 1e-3  \n",
    "MINIBATCH_SIZE = 64\n",
    "E_DECAY = 0.995\n",
    "E_MIN = 0.01\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Environment\n",
    "\n",
    "##### Observation Space\n",
    "The agent's observation space consists of a state vector with 8 variables:\n",
    "* Its $(x,y)$ coordinates. The landing pad is always at coordinates $(0,0)$.\n",
    "* Its linear velocities $(\\dot x,\\dot y)$.\n",
    "* Its angle $\\theta$.\n",
    "* Its angular velocity $\\dot \\theta$.\n",
    "* Two booleans, $l$ and $r$, that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "##### Reward\n",
    "The Lunar Lander environment has the following reward system:\n",
    "* Landing on the landing pad and coming to rest is about 100-140 points.\n",
    "* If the lander moves away from the landing pad, it loses reward. \n",
    "* If the lander crashes, it receives -100 points.\n",
    "* If the lander comes to rest, it receives +100 points.\n",
    "* Each leg with ground contact is +10 points.\n",
    "* Firing the main engine is -0.3 points each frame.\n",
    "* Firing the side engine is -0.03 points each frame.\n",
    "\n",
    "##### Action Space\n",
    "Do nothing = 0, Fire right engine = 1, main engine = 2, left = 3\n",
    "\n",
    "##### Terminal State\n",
    "An episode ends (i.e the environment enters a terminal state) if:\n",
    "\n",
    "* The lunar lander crashes (i.e if the body of the lunar lander comes in contact with the surface of the moon).\n",
    "* The lander's $x$-coordinate is greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: (8,)\n",
      "Number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(random.seed(SEED))\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "# get info about environment: \n",
    "# size of state vector and number of valid actions\n",
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print(f\"State size: {state_size}\")\n",
    "print(f\"Number of actions: {num_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: (array([-0.008, 1.412, -0.803, 0.028, 0.009, 0.182, 0.000, 0.000],\n",
      "      dtype=float32), {})\n",
      "Action: 0\n",
      "Next State: [-0.016 1.412 -0.802 0.002 0.018 0.180 0.000 0.000]\n",
      "Reward Received: -0.754248141704295\n",
      "Episode Terminated: False\n",
      "Info: {}\n"
     ]
    }
   ],
   "source": [
    "initial_state = env.reset()\n",
    "action = 0\n",
    "\n",
    "# Run a single time step of the environment's dynamics with the given action\n",
    "next_state, reward, done, _, info = env.step(action)\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.3f}'.format}):\n",
    "    print(\"Initial State:\", initial_state)\n",
    "    print(\"Action:\", action)\n",
    "    print(\"Next State:\", next_state)\n",
    "    print(\"Reward Received:\", reward)\n",
    "    print(\"Episode Terminated:\", done)\n",
    "    print(\"Info:\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network\n",
    "\n",
    "In cases where both the state and action space are discrete we can estimate the action-value function iteratively by using the Bellman equation:\n",
    "\n",
    "$$\n",
    "Q_{i+1}(s,a) = R + \\gamma \\max_{a'}Q_i(s',a')\n",
    "$$\n",
    "\n",
    "This iterative method converges to the optimal action-value function $Q^*(s, a)$ as $i \\to\\infty$, which means if the agent gradually explore the state-action space and keep updating its estimate the Q function will converge to the optimal. However, for continous state-action space, you can't explore the entire space so you need to use a neural network to estimate the action-value function $Q(s,a) \\approx Q^*(s,a)$. This is called a Q-network and it can be trained by adjusting weights at each iteration to minimize the MSE in Bellman Eq.\n",
    "\n",
    "Below are two methods to avoid its instabilities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Target network \n",
    "\n",
    "The target values are given by: \n",
    "$$\n",
    "y = R + \\gamma \\max_{a'}Q(s',a';w)\n",
    "$$\n",
    "\n",
    "We are adjusting the weights w to minimize the following error:\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}Q(s',a'; w)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "The fact that y target is changing on every iteration is a problem. We need to create a separate neural network for generating y targets. It is called the **target $\\hat Q$-Network** and will have the same architecture as the original Q network. The error is now:\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}\\hat{Q}(s',a'; w^-)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "* Every C time steps we use target network to generate y targets\n",
    "* Then update weights of target Q network using weights of Q network\n",
    "* We will use a soft update:\n",
    "$$\n",
    "w^-\\leftarrow \\tau w + (1 - \\tau) w^-\n",
    "$$\n",
    "\n",
    "In this exercise you will create the $Q$ and target $\\hat Q$ networks and set the optimizer. Remember that the Deep $Q$-Network (DQN) is a neural network that approximates the action-value function $Q(s,a)\\approx Q^*(s,a)$. It does this by learning how to map states to $Q$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = Sequential([\n",
    "    Input(state_size), \n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_actions, activation='linear')\n",
    "])\n",
    "\n",
    "target_q_network = Sequential([\n",
    "    Input(state_size),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_actions, activation='linear')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=ALPHA)\n",
    "target_q_network.set_weights(q_network.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Experience Replay\n",
    "\n",
    "* s, a, reward the agent experiences are sequential by nature -- strong correlations = problem in learning\n",
    "    * Experience replay generates uncorrelated experiences for training the agent\n",
    "    * This avoid problematic correlations, oscillations, and instabilities\n",
    "* Store the agent's experiences (s, a, reward receives) in a memory buffer \n",
    "* Then sample a random mini-batch of experiences form the buffer for learning\n",
    "\n",
    "The experience tuples $(S_t, A_t, R_t, S_{t+1})$ will be added to the memory buffer at each time step as the agent interacts with the environment. We will use named tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "# There is a good picture in Notion that summarizes the whole DQL algorithm with Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Learning Algorithm (with Experience Replay)\n",
    "\n",
    "To compute the loss between the y targets and $Q(s,a)$ values, we will set the target euqal to:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y_j =\n",
    "    \\begin{cases}\n",
    "      R_j & \\text{if episode terminates at step  } j+1\\\\\n",
    "      R_j + \\gamma \\max_{a'}\\hat{Q}(s_{j+1},a') & \\text{otherwise}\\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The compute_loss function will take a mini-batch of experience tuples, unpacked to extract its elements which are tf Tensors with size = size of mini-batch. i.e. if mini-batch size = 64 then \"rewards\" extracted will be tf Tensor of 64 elements. The MSE loss is imported from keras but ofc you can implement it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line 12 of the algorithm\n",
    "\n",
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \n",
    "    # Unpack the mini-batch of experience tuple\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "    # Compute max Q^(s, a) for the next states using the target network\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states, training=False), axis = -1)\n",
    "\n",
    "    # Set y (target values) = R if episode terminates, otherwise y = R + gamma * max Q^(s, a)\n",
    "    y_targets = rewards + (gamma * max_qsa * (1 - dones))\n",
    "\n",
    "    # Get the Q values for the current state\n",
    "    q_values = q_network(states)\n",
    "\n",
    "    # Get the Q values for the actions taken\n",
    "    action_q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]), tf.cast(actions, tf.int32)], axis = 1))\n",
    "\n",
    "    loss = MSE(y_targets, action_q_values)\n",
    "\n",
    "    return loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will update the weights of networks $Q$ and $\\hat Q$ using a custom training loop. Therefore we need to retrieve the gradients via a `GradientTape` instance, and then call `optimizer.apply_gradients()` to update our Q-network.  The `@tf.function` decorator can increase performance and shorten training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line 12 - 14 of the algorithm\n",
    "\n",
    "@tf.function # This is just for faster training\n",
    "def agent_learn(experiences, gamma):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "    \n",
    "    # Autodifferentiation: find gradients of loss with respect to weights\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "\n",
    "    # Apply the gradients to the optimizer to update the weights of q_network\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target q_network using soft update from q network\n",
    "    update_target_network(TAU)\n",
    "\n",
    "def update_target_network(tau):\n",
    "    for target_weights, q_net_weights in zip(target_q_network.weights, q_network.weights):\n",
    "        target_weights.assign(tau * q_net_weights + (1-tau) * target_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent (Putting it all together)\n",
    "1. Initialize memory buffer with capacity N = MEMORY_SIZE using a deque as data structure\n",
    "\n",
    "2. Initialize q_network (done)\n",
    "\n",
    "3. Initialize target_q_network by setting its weights equal to q_network (done)\n",
    "\n",
    "4. Start outer loop: M = num_episodes = 2000 in which the agent should solve the environment\n",
    "\n",
    "5. Use .reset() method to reset environment to initial state (for each new episode)\n",
    "\n",
    "6. Star inner loop: T = max_num_timesteps = 1000\n",
    "\n",
    "7. Agent observes current state and choose action using epsilon-greedy policy, set epislon initially as 1 (random) and gradually decrease (decay rate) to a min of 0.01 -- `get_action()`\n",
    "\n",
    "8. Use the .step() method to take the given action in the environment and get reward and next_state\n",
    "\n",
    "9. Store experience(state, action, reward, next_state, done) to memory_buffer\n",
    "\n",
    "10. Check if conditions met to perform a learning udpate (so not too frequently, in `check_update_conditions`)\n",
    "    \n",
    "    a. C = NUM_STEPS_FOR_UPDATE = 4 time steps have occured\n",
    "    \n",
    "    b. Memory_buffer has enough experience tuples to fill a mini-batch sample\n",
    "\n",
    "11. If update is True, perform learning update: sample mini-batch randomly, set y targets and calculate loss, perform gradient descent, update weights of the networks -- `agent_learn()` (done)\n",
    "\n",
    "15. End of iteration of inner loop: \n",
    "    \n",
    "    a. Set next_state as our new state so the loop can start again from this new state\n",
    "    \n",
    "    b. Check if episodes has reached a terminal state (done = True) -> break out of inner loop\n",
    "\n",
    "16. End of outer loop iteration: \n",
    "    \n",
    "    a. Update value of epsilon \n",
    "    \n",
    "    b. Check if environment has been solved -- agent receives avg 200 points in the last 100 episodes\n",
    "    \n",
    "    c. If not solved, continue outer loop and start a new episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(q_values, epsilon):\n",
    "    if random.random() < epsilon: \n",
    "        return random.randint(0, num_actions - 1)\n",
    "    else:\n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "def check_update_conditions(t, num_steps_for_update, memory):\n",
    "    if t % num_steps_for_update == 0 and len(memory) >= MINIBATCH_SIZE:\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def get_experiences(memory):\n",
    "    experiences = random.sample(memory, MINIBATCH_SIZE)\n",
    "    \n",
    "    states = tf.convert_to_tensor(np.array([e.state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(np.array([e.action for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    rewards = tf.convert_to_tensor(np.array([e.reward for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    next_states = tf.convert_to_tensor(np.array([e.next_state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    dones = tf.convert_to_tensor(np.array([e.done for e in experiences if e is not None]).astype(np.uint8),dtype=tf.float32)\n",
    "\n",
    "    return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "def get_new_eps(epsilon):\n",
    "    return max(E_MIN, epsilon * E_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 21:13:32.808500: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 | Total point average of the last 100 episodes: -150.47\n",
      "Episode 200 | Total point average of the last 100 episodes: -92.40\n",
      "Episode 300 | Total point average of the last 100 episodes: -19.58\n",
      "Episode 400 | Total point average of the last 100 episodes: 30.40\n",
      "Episode 500 | Total point average of the last 100 episodes: 136.06\n",
      "Episode 600 | Total point average of the last 100 episodes: 181.03\n",
      "Episode 700 | Total point average of the last 100 episodes: 197.47\n",
      "\n",
      "Environment solved in 702 episodes!\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "\n",
      "Total Runtime: 1506.01 s (25.10 min)\n"
     ]
    }
   ],
   "source": [
    "# Putting it all together:\n",
    "start = time.time()\n",
    "\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1000\n",
    "\n",
    "num_p_av = 100 # Number of episodes to average over, we need last 100 episodes to be > 200\n",
    "epsilon = 1.0\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_points = 0\n",
    "\n",
    "    for t in range(max_num_timesteps):\n",
    "\n",
    "        # From current state S choose an action A using epsilon-greedy policy\n",
    "        state_qn = np.expand_dims(state, axis=0) # reshape state\n",
    "        q_values = q_network(state_qn, training=False)\n",
    "        action = get_action(q_values, epsilon)\n",
    "\n",
    "        # Take action A and receive reward R and the next state S'\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "\n",
    "        update = check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "\n",
    "        if update:\n",
    "            # random sample mini-batch\n",
    "            experiences = get_experiences(memory_buffer)\n",
    "            agent_learn(experiences, GAMMA)\n",
    "\n",
    "        state = next_state.copy()\n",
    "        total_points += reward\n",
    "\n",
    "        if done: break\n",
    "    \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:]) # Last 100 episodes\n",
    "\n",
    "    epsilon = get_new_eps(epsilon)\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "    \n",
    "    # if last 100 episodes avg >= 200 points, we can end\n",
    "    if av_latest_points >= 200:\n",
    "        print(f\"\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save(\"lunar_lander_model.keras\")\n",
    "        break\n",
    "\n",
    "tot_time = time.time() - start\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
