{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will identify if a mushroom is poisonous or not using the following training data of 10 mushrooms:\n",
    "\n",
    "| Cap Color | Stalk Shape | Solitary | Edible |\n",
    "|:---------:|:-----------:|:--------:|:------:|\n",
    "|   Brown   |   Tapering  |    Yes   |    1   |\n",
    "|   Brown   |  Enlarging  |    Yes   |    1   |\n",
    "|   Brown   |  Enlarging  |    No    |    0   |\n",
    "|   Brown   |  Enlarging  |    No    |    0   |\n",
    "|   Brown   |   Tapering  |    Yes   |    1   |\n",
    "|    Red    |   Tapering  |    Yes   |    0   |\n",
    "|    Red    |  Enlarging  |    No    |    0   |\n",
    "|   Brown   |  Enlarging  |    Yes   |    1   |\n",
    "|    Red    |   Tapering  |    No    |    1   |\n",
    "|   Brown   |  Enlarging  |    No    |    0   |\n",
    "\n",
    "First one-hot encode the features:\n",
    "\n",
    "| Brown Cap | Tapering Stalk Shape | Solitary | Edible |\n",
    "|:---------:|:--------------------:|:--------:|:------:|\n",
    "|     1     |           1          |     1    |    1   |\n",
    "|     1     |           0          |     1    |    1   |\n",
    "|     1     |           0          |     0    |    0   |\n",
    "|     1     |           0          |     0    |    0   |\n",
    "|     1     |           1          |     1    |    1   |\n",
    "|     0     |           1          |     1    |    0   |\n",
    "|     0     |           0          |     0    |    0   |\n",
    "|     1     |           0          |     1    |    1   |\n",
    "|     0     |           1          |     0    |    1   |\n",
    "|     1     |           0          |     0    |    0   |\n",
    "\n",
    "- `X_train` contains three features for each sample\n",
    "- `y_train` is whether the mushroom is edible (1 = edible, 0 = poisonous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\n",
    "y_train = np.array([1,1,0,0,1,0,0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy at root node:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Takes a numpy array that indicates whether the example is edible or poisonous\n",
    "# Compute p1 which is the fraction that are edible and calculate entropy H(p1)\n",
    "\n",
    "def compute_entropy(y):\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    p1 = np.sum(y)/len(y)\n",
    "    if p1 == 0 or p1 == 1: # to handle 0log0\n",
    "        return 0\n",
    "    return -p1*np.log2(p1) - (1-p1)*np.log2(1-p1)\n",
    "\n",
    "print(\"Entropy at root node: \", compute_entropy(y_train)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refer to each example using an index and each feature as well. For example, brown cap would be feature 0, and if the value is 0 it means it has a red cap.\n",
    "\n",
    "For each time splitting the data, we will not use the actual data but instead indices referencing to the X_train dataset. This allows for recursion since we don't always work with the root_node.\n",
    "\n",
    "| Index | Brown Cap | Tapering Stalk Shape | Solitary | Edible |\n",
    "|:-----:|:---------:|:--------------------:|:--------:|:------:|\n",
    "|   0   |     1     |           1          |     1    |    1   |\n",
    "|   1   |     1     |           0          |     1    |    1   |\n",
    "|   2   |     1     |           0          |     0    |    0   |\n",
    "|   3   |     1     |           0          |     0    |    0   |\n",
    "|   4   |     1     |           1          |     1    |    1   |\n",
    "|   5   |     0     |           1          |     1    |    0   |\n",
    "|   6   |     0     |           0          |     0    |    0   |\n",
    "|   7   |     1     |           0          |     1    |    1   |\n",
    "|   8   |     0     |           1          |     0    |    1   |\n",
    "|   9   |     1     |           0          |     0    |    0   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Refresher\n",
    "\n",
    "- Start with all examples at the root node\n",
    "- Calculate information gain for splitting on all possible features, and pick the highest\n",
    "- Split dataset according to the selected feature, and create left and right branches\n",
    "- Keep repeating splitting process until stopping criteria is met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left indices:  [0, 1, 2, 3, 4, 7, 9]\n",
      "Right indices:  [5, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "# Using indices is more efficient than using the actual data\n",
    "# This function will work with ANY subset of the data just using indices\n",
    "\n",
    "def split_dataset(X, node_indices, feature_index):\n",
    "    left_indices, right_indices = [], []\n",
    "    for i in node_indices:\n",
    "        if X[i][feature_index] == 0:\n",
    "            right_indices.append(i)\n",
    "        else:\n",
    "            left_indices.append(i)\n",
    "    return left_indices, right_indices\n",
    "\n",
    "root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices, feature_index=0)\n",
    "\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain from splitting the root on brown cap:  0.034851554559677034\n"
     ]
    }
   ],
   "source": [
    "def compute_information_gain(X, y, node_indices, feature_index):\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature_index)\n",
    "    H_node = compute_entropy(y[node_indices])\n",
    "    H_left, H_right = compute_entropy(y[left_indices]), compute_entropy(y[right_indices])\n",
    "    w_left = len(left_indices)/len(node_indices)\n",
    "    w_right = len(right_indices)/len(node_indices)\n",
    "    return H_node - (w_left*H_left + w_right*H_right)\n",
    "\n",
    "info_gain0 = compute_information_gain(X_train, y_train, root_indices, feature_index=0)\n",
    "print(\"Information Gain from splitting the root on brown cap: \", info_gain0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature to split on: 2\n"
     ]
    }
   ],
   "source": [
    "def get_best_split(X, y, node_indices):\n",
    "    features = X.shape[1]\n",
    "    best_feature = -1\n",
    "    best_gain = 0\n",
    "    for i in range(features):\n",
    "        info_gain = compute_information_gain(X, y, node_indices, i)\n",
    "        if info_gain > best_gain:\n",
    "            best_gain = info_gain\n",
    "            best_feature = i\n",
    "    return best_feature\n",
    "\n",
    "best_feature = get_best_split(X_train, y_train, root_indices)\n",
    "print(\"Best feature to split on: %d\" % best_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we will use a more advanced implementation to build and use the decision tree. **Please make sure you fully understand the code below, it is a good data structure and recursion practice!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting on feature 2\n",
      "Splitting on feature 0\n",
      "Leaf node\n",
      "Leaf node\n",
      "Splitting on feature 1\n",
      "Leaf node\n",
      "Leaf node\n"
     ]
    }
   ],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature, indices, class_prediction=None):\n",
    "        self.feature = feature # if this is none then it is a leaf node\n",
    "        self.indices = indices\n",
    "        self.class_prediction = class_prediction\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "# the stopping criteria we use is when maximum depth is 2\n",
    "def build_tree_recursive(X, y, node_indices, max_depth, depth):\n",
    "    if depth == max_depth:\n",
    "        leaf_value = int(np.sum(y_train[node_indices]) > len(node_indices)/2)\n",
    "        return Node(None, node_indices, class_prediction=leaf_value)\n",
    "    \n",
    "    feature = get_best_split(X, y, node_indices)\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "\n",
    "    node = Node(feature, node_indices)\n",
    "    node.left = build_tree_recursive(X, y, left_indices, max_depth, depth + 1)\n",
    "    node.right = build_tree_recursive(X, y, right_indices, max_depth, depth + 1)\n",
    "\n",
    "    return node # return the root node\n",
    "\n",
    "root_node = build_tree_recursive(X_train, y_train, root_indices, max_depth=2, depth=0)\n",
    "\n",
    "def print_tree(node):\n",
    "    if node.feature is None:\n",
    "        print(\"Leaf node\")\n",
    "    else: \n",
    "        print(\"Splitting on feature %d\" % node.feature)\n",
    "        print_tree(node.left)\n",
    "        print_tree(node.right)\n",
    "\n",
    "print_tree(root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [1. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# now we use the built tree to make predictions\n",
    "\n",
    "def predict(node, x):\n",
    "    if node.feature is None:\n",
    "        return node.class_prediction\n",
    "\n",
    "    if x[node.feature] == 0: # very simple splitting rule!\n",
    "        return predict(node.right, x)\n",
    "    else:\n",
    "        return predict(node.left, x)\n",
    "    \n",
    "X_test = np.array([[1,1,0],[1,0,1],[0,1,1],[0,0,0],[1,0,0]])\n",
    "y_pred = np.zeros(len(X_test))\n",
    "for i, x in enumerate(X_test):\n",
    "    y_pred[i] = predict(root_node, x)\n",
    "\n",
    "print(\"Predictions: \", y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
